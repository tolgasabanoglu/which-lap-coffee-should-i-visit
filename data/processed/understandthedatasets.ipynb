{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lst from /Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed/lap_locations_LST_1km_complete.gpkg ...\n",
      "lst: 3304 rows, columns: ['name', 'address', 'lat', 'lon', 'date', 'lst_celsius_1km', 'geometry']\n",
      "\n",
      "Loading weather from /Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed/lap_locations_historical_weather_multi_year.gpkg ...\n",
      "weather: 3920 rows, columns: ['weather_date', 'temp_max', 'temp_min', 'precip_mm', 'name', 'address', 'lat', 'lon', 'rating', 'user_ratings_total', 'season', 'geometry']\n",
      "\n",
      "Loading ndvi from /Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed/lap_locations_ndvi_daily_multi_year.gpkg ...\n",
      "ndvi: 3920 rows, columns: ['name', 'address', 'lat', 'lon', 'date', 'ndvi', 'geometry']\n",
      "\n",
      "Loading nightlights from /Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed/lap_locations_nightlights_monthly_multi_year.gpkg ...\n",
      "nightlights: 3920 rows, columns: ['name', 'address', 'lat', 'lon', 'date', 'nightlight', 'geometry']\n",
      "\n",
      "Loading open_bars from /Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed/lap_locations_with_open_bars.gpkg ...\n",
      "open_bars: 16 rows, columns: ['name', 'address', 'lat', 'lon', 'rating', 'user_ratings_total', 'place_id', 'open_bars_count_500m', 'geometry']\n",
      "\n",
      "Loading parks from /Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed/lap_locations_with_park_counts.gpkg ...\n",
      "parks: 16 rows, columns: ['name', 'address', 'lat', 'lon', 'rating', 'user_ratings_total', 'place_id', 'parks_count_1km', 'geometry']\n",
      "\n",
      "### lst\n",
      "Date range: 2023-09-01 - 2025-10-15\n",
      "\n",
      "\n",
      "### weather\n",
      "Weather date range: 2023-09-01 - 2025-11-02\n",
      "\n",
      "\n",
      "### ndvi\n",
      "Date range: 2023-09-01 - 2025-11-02\n",
      "\n",
      "\n",
      "### nightlights\n",
      "Date range: 2023-09-01 - 2025-11-02\n",
      "\n",
      "\n",
      "### open_bars\n",
      "\n",
      "\n",
      "### parks\n",
      "\n",
      "\n",
      "Removing duplicate columns: ['rating_parks', 'user_ratings_total_parks', 'rating_open_bars', 'user_ratings_total_open_bars', 'place_id_open_bars']\n",
      "Required date range covers a total of 245 unique days across all specified periods.\n",
      "‚úÖ Expanded dataset covers 245 days √ó 16 cafes = 3,920 rows\n",
      "‚úÖ Merged dataset saved: /Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed/lap_locations_final_merged.csv\n",
      "Columns: ['date', 'name', 'lat', 'lon', 'address', 'cafe_rating', 'cafe_user_ratings_total', 'cafe_place_id', 'weather_date', 'season', 'parks_count_1km', 'open_bars_count_500m', 'lst_celsius_1km', 'geometry', 'temp_max', 'temp_min', 'precip_mm', 'ndvi', 'nightlight']\n",
      "Number of rows: 3920\n",
      "Sample rows:\n",
      "          date        name        lat       lon  \\\n",
      "0  2024-09-01  LAP COFFEE  52.486768  13.35549   \n",
      "1  2024-09-02  LAP COFFEE  52.486768  13.35549   \n",
      "2  2024-09-03  LAP COFFEE  52.486768  13.35549   \n",
      "3  2024-09-04  LAP COFFEE  52.486768  13.35549   \n",
      "4  2024-09-05  LAP COFFEE  52.486768  13.35549   \n",
      "\n",
      "                                   address  cafe_rating  \\\n",
      "0  Akazienstra√üe 3A, 10823 Berlin, Germany          4.7   \n",
      "1  Akazienstra√üe 3A, 10823 Berlin, Germany          4.7   \n",
      "2  Akazienstra√üe 3A, 10823 Berlin, Germany          4.7   \n",
      "3  Akazienstra√üe 3A, 10823 Berlin, Germany          4.7   \n",
      "4  Akazienstra√üe 3A, 10823 Berlin, Germany          4.7   \n",
      "\n",
      "   cafe_user_ratings_total                cafe_place_id weather_date  season  \\\n",
      "0                      151  ChIJGzWI8E5RqEcRYF2oui3pSKc   2024-09-01  Autumn   \n",
      "1                      151  ChIJGzWI8E5RqEcRYF2oui3pSKc   2024-09-01  Autumn   \n",
      "2                      151  ChIJGzWI8E5RqEcRYF2oui3pSKc   2024-09-01  Autumn   \n",
      "3                      151  ChIJGzWI8E5RqEcRYF2oui3pSKc   2024-09-01  Autumn   \n",
      "4                      151  ChIJGzWI8E5RqEcRYF2oui3pSKc   2024-09-01  Autumn   \n",
      "\n",
      "   parks_count_1km  open_bars_count_500m  lst_celsius_1km  \\\n",
      "0               11                    12            30.89   \n",
      "1               11                    12            30.89   \n",
      "2               11                    12            30.89   \n",
      "3               11                    12            30.89   \n",
      "4               11                    12            31.15   \n",
      "\n",
      "                    geometry  temp_max  temp_min  precip_mm      ndvi  \\\n",
      "0  POINT (13.35549 52.48677)      25.2      13.9        0.0  0.254739   \n",
      "1  POINT (13.35549 52.48677)      26.7      12.7        0.0  0.254739   \n",
      "2  POINT (13.35549 52.48677)      32.6      18.0        0.2  0.254739   \n",
      "3  POINT (13.35549 52.48677)      33.9      20.2        0.0  0.254739   \n",
      "4  POINT (13.35549 52.48677)      31.1      21.5        0.0  0.273590   \n",
      "\n",
      "   nightlight  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Explore and Merge LAP Coffee Datasets (with PM2.5 and Parks)\n",
    "# \n",
    "# This notebook will:\n",
    "# 1. Load all GeoPackages\n",
    "# 2. Inspect structure and date ranges\n",
    "# 3. Merge daily and static datasets (including PM2.5 and parks)\n",
    "# 4. Keep only nearest park per address (NOTE: This logic is not implemented in the provided code, but this script will retain all park rows initially if they exist).\n",
    "# 5. Ensure full daily coverage from 2025-01-01 -> **MODIFIED to cover specific multi-year ranges.**\n",
    "# 6. Deduplicate rating/user_ratings_total/place_id columns\n",
    "# 7. Save final CSV for dbt\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1Ô∏è‚É£ Import libraries\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime # Import datetime for date range definition\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2Ô∏è‚É£ Define file paths\n",
    "data_dir = Path(\"/Users/tolgasabanoglu/Desktop/github/which-lap-coffee-should-i-visit/data/processed\")\n",
    "\n",
    "gpkg_files = {\n",
    "    \"lst\": data_dir / \"lap_locations_LST_1km_complete.gpkg\",  # PM2.5\n",
    "    \"weather\": data_dir / \"lap_locations_historical_weather_multi_year.gpkg\",\n",
    "    \"ndvi\": data_dir / \"lap_locations_ndvi_daily_multi_year.gpkg\",\n",
    "    \"nightlights\": data_dir / \"lap_locations_nightlights_monthly_multi_year.gpkg\",\n",
    "    # UPDATED: File name uses 'lap_locations_with_all_bars.gpkg'\n",
    "    \"open_bars\": data_dir / \"lap_locations_with_open_bars.gpkg\", \n",
    "    \"parks\": data_dir / \"lap_locations_with_park_counts.gpkg\"  # Parks\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üåü NEW: Define the specific date ranges to cover\n",
    "# ----------------------------------------------------------------------\n",
    "TARGET_DATE_RANGES = [\n",
    "    (\"2024-09-01\", \"2024-11-30\"),\n",
    "    (\"2023-09-01\", \"2023-11-30\"),\n",
    "    # Use the current time for the end date of the 2025 range, as defined in your scripts\n",
    "    (\"2025-09-01\", \"2025-11-02\"), \n",
    "]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3Ô∏è‚É£ Load GeoPackages\n",
    "gdfs = {}\n",
    "for name, path in gpkg_files.items():\n",
    "    print(f\"Loading {name} from {path} ...\")\n",
    "    try:\n",
    "        gdfs[name] = gpd.read_file(path, layer=\"lap_coffee\")\n",
    "        print(f\"{name}: {gdfs[name].shape[0]} rows, columns: {list(gdfs[name].columns)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Could not load {name} from {path}. Skipping this file. Error: {e}\\n\")\n",
    "        # Ensure the key is removed if loading fails to prevent KeyErrors later\n",
    "        if name in gpkg_files:\n",
    "            del gpkg_files[name]\n",
    "        if name in gdfs:\n",
    "            del gdfs[name]\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4Ô∏è‚É£ Check date ranges\n",
    "for name, gdf in gdfs.items():\n",
    "    print(f\"### {name}\")\n",
    "    if \"date\" in gdf.columns:\n",
    "        print(\"Date range:\", gdf[\"date\"].min(), \"-\", gdf[\"date\"].max())\n",
    "    if \"weather_date\" in gdf.columns:\n",
    "        print(\"Weather date range:\", gdf[\"weather_date\"].min(), \"-\", gdf[\"weather_date\"].max())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5Ô∏è‚É£ Normalize date columns\n",
    "if \"weather\" in gdfs and \"weather_date\" in gdfs[\"weather\"].columns:\n",
    "    gdfs[\"weather\"][\"date\"] = pd.to_datetime(gdfs[\"weather\"][\"weather_date\"]).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "for key in [\"lst\", \"ndvi\", \"nightlights\"]:\n",
    "    if key in gdfs and \"date\" in gdfs[key].columns:\n",
    "        gdfs[key][\"date\"] = pd.to_datetime(gdfs[key][\"date\"]).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7Ô∏è‚É£ Merge daily datasets (lst, weather, ndvi, nightlights)\n",
    "# Using \"lst\" key for the initial merge (PM2.5).\n",
    "if \"lst\" not in gdfs:\n",
    "    raise FileNotFoundError(\"Cannot proceed: 'lst' data is missing or failed to load.\")\n",
    "    \n",
    "daily_merged = gdfs[\"lst\"].copy()\n",
    "\n",
    "for name in [\"weather\", \"ndvi\", \"nightlights\"]:\n",
    "    if name in gdfs:\n",
    "        merge_df = gdfs[name].drop(columns=[\"geometry\", \"address\"], errors=\"ignore\")\n",
    "        daily_merged = daily_merged.merge(\n",
    "            merge_df,\n",
    "            on=[\"name\", \"lat\", \"lon\", \"date\"],\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", f\"_{name}\")\n",
    "        )\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8Ô∏è‚É£ Merge static datasets (parks, open_bars)\n",
    "# Only includes parks, and open_bars.\n",
    "for name in [\"parks\", 'open_bars']:\n",
    "    if name in gdfs:\n",
    "        merge_df = gdfs[name].drop(columns=[\"geometry\", \"address\"], errors=\"ignore\")\n",
    "        # The parks data might have multiple rows per cafe (for different parks). \n",
    "        # This merge keeps all of them, but the final required output needs to address the \"nearest park\" requirement.\n",
    "        daily_merged = daily_merged.merge(\n",
    "            merge_df,\n",
    "            on=[\"name\", \"lat\", \"lon\"],\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", f\"_{name}\")\n",
    "        )\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9Ô∏è‚É£ Clean duplicate metadata columns\n",
    "duplicate_cols = [c for c in daily_merged.columns if any(x in c for x in [\"rating_\", \"user_ratings_total_\", \"place_id_\"])]\n",
    "\n",
    "if duplicate_cols:\n",
    "    print(\"Removing duplicate columns:\", duplicate_cols)\n",
    "    daily_merged = daily_merged.drop(columns=duplicate_cols, errors=\"ignore\")\n",
    "\n",
    "rename_map = {\n",
    "    \"rating\": \"cafe_rating\",\n",
    "    \"user_ratings_total\": \"cafe_user_ratings_total\",\n",
    "    \"place_id\": \"cafe_place_id\"\n",
    "}\n",
    "daily_merged = daily_merged.rename(columns=rename_map)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üîü Generate geometry column\n",
    "daily_merged = gpd.GeoDataFrame(\n",
    "    daily_merged,\n",
    "    geometry=gpd.points_from_xy(daily_merged.lon, daily_merged.lat),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1Ô∏è‚É£1Ô∏è‚É£ Ensure full date coverage per caf√© (Multi-Year Range)\n",
    "daily_merged[\"date\"] = pd.to_datetime(daily_merged[\"date\"])\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üåü FINAL FIX: Generate the exact union of all required dates\n",
    "# ----------------------------------------------------------------------\n",
    "all_dates_list = []\n",
    "for start, end in TARGET_DATE_RANGES:\n",
    "    all_dates_list.append(pd.date_range(start, end))\n",
    "\n",
    "# üí° FIX: Convert each DatetimeIndex to a Series and use pd.concat, \n",
    "# which is the most robust way to merge these types of objects across versions.\n",
    "all_dates = pd.concat([pd.Series(idx) for idx in all_dates_list]).dt.normalize().unique()\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(f\"Required date range covers a total of {len(all_dates)} unique days across all specified periods.\")\n",
    "\n",
    "# Identify unique cafes and static columns\n",
    "cafes_static_cols = ['name', 'lat', 'lon', 'address', 'cafe_rating', 'cafe_user_ratings_total', 'cafe_place_id']\n",
    "# Also include static park/bar columns that are now duplicated across daily rows\n",
    "static_cols_to_keep = [\n",
    "    col for col in daily_merged.columns \n",
    "    if col not in ['date', 'lst_celsius_1km', 'temp_max', 'temp_min', 'precip_mm', 'ndvi', 'nightlight', 'geometry'] and col not in cafes_static_cols\n",
    "]\n",
    "\n",
    "\n",
    "cafes = daily_merged[cafes_static_cols + static_cols_to_keep].drop_duplicates(subset=['name', 'lat', 'lon']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create the Cartesian product (full index)\n",
    "full_index = pd.MultiIndex.from_product([cafes.index, all_dates], names=[\"cafe_idx\", \"date\"])\n",
    "full_df = pd.DataFrame(index=full_index).reset_index()\n",
    "\n",
    "# Merge static cafe metadata back into the full date frame\n",
    "full_df = full_df.merge(cafes.reset_index().rename(columns={'index': 'cafe_idx_merge'}), \n",
    "                        left_on=\"cafe_idx\", \n",
    "                        right_on=\"cafe_idx_merge\", \n",
    "                        how=\"left\")\n",
    "full_df = full_df.drop(columns=[\"cafe_idx\", \"cafe_idx_merge\"])\n",
    "\n",
    "# Ensure date is datetime for final merge\n",
    "full_df[\"date\"] = pd.to_datetime(full_df[\"date\"])\n",
    "\n",
    "# Merge the original, sparse merged data onto the complete date frame\n",
    "daily_merged = full_df.merge(\n",
    "    # Drop static columns from the merged data before the final merge to avoid redundant columns/data\n",
    "    daily_merged.drop(columns=static_cols_to_keep + ['cafe_rating', 'cafe_user_ratings_total', 'cafe_place_id', 'address'], errors='ignore'),\n",
    "    on=[\"name\", \"lat\", \"lon\", \"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Expanded dataset covers {len(all_dates)} days √ó {len(cafes)} cafes = {len(all_dates) * len(cafes):,} rows\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1Ô∏è‚É£2Ô∏è‚É£ Convert date to string (for dbt / CSV)\n",
    "daily_merged[\"date\"] = daily_merged[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1Ô∏è‚É£3Ô∏è‚É£ Save final merged dataset\n",
    "output_csv = data_dir / \"lap_locations_final_merged.csv\"\n",
    "daily_merged.to_csv(output_csv, index=False)\n",
    "print(f\"‚úÖ Merged dataset saved: {output_csv}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1Ô∏è‚É£4Ô∏è‚É£ Quick summary\n",
    "print(\"Columns:\", list(daily_merged.columns))\n",
    "print(\"Number of rows:\", daily_merged.shape[0])\n",
    "print(\"Sample rows:\\n\", daily_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lapgee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
